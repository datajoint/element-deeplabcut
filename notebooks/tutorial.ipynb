{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DataJoint Element DeepLabCut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-source data pipeline to automate analyses and organize data.\n",
    "\n",
    "In this tutorial, we will walk through creating, testing, and analyzing a pose estimation model using DeepLabCut.\n",
    "\n",
    "For detailed documentation and tutorials on general DataJoint principles that support collaboration, automation, reproducibility, and visualizations:\n",
    "\n",
    "[`DataJoint for Python - Interactive Tutorials`](https://github.com/datajoint/datajoint-tutorials) - Fundamentals including table tiers, query operations, fetch operations, automated computations with the make function, etc.\n",
    "\n",
    "[`DataJoint for Python - Documentation`](https://datajoint.com/docs/core/datajoint-python/0.14/)\n",
    "\n",
    "[`DataJoint Element for DeepLabCut - Documentation`](https://datajoint.com/docs/elements/element-deeplabcut/0.2/)"
   ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Sample dataset in a DeepLabCut project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notebooks are built around data provided by DataJoint, including a well-trained model. \n",
    "\n",
    "We will use the following dataset `DeepLabCut project example` as an example across this tutorial. You can download this project example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Configuring DataJoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to `configure the connection` of the DataJoint schemas to the same database server with the same user credentials. \n",
    "\n",
    "- If this is the first time that you are running this tutorial, then you will need to specify the connection parameters by input arguments in `1.2.1. Configuration Code for Initiating this Tutorial`. To prevent this to be necessary for further analysis in this tutorial, in the next step we will create a DataJoint configuration file named `dj_local_conf.json` that will save these credentials arguments as environment variables (DJ_HOST, DJ_USER, DJ_PASS). Thus, the configuration file is unique to each machine and will be created in your `Element-Deeplabcut` directory.\n",
    "\n",
    "- If you have already run this tutorial and created the `.json` file with your credentials info, then you can directly jump to the `1.2.2. Configuration code to configure this tutorial in subsequent restarts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1. Configuration Code for Initiating This Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *The configuration file only needs to be set up once. If you already have one, jump to the following subsection `1.2.2. Configuration code to configure this tutorial in subsequent restarts`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By convention, we set a local config in the workflow directory.\n",
    "import os\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='element-deeplabcut', (\"Please move to the \"\n",
    "                                                              + \"element directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages necessary to run this DataJoint pipeline `Element-DeepLabCut`\n",
    "import datajoint as dj\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection parameters are specified by input arguments:\n",
    "- HOST, USER, AND PASSWORD are the fields for the user credentials\n",
    "- Configuring a `custom` field helps manage privileges on a server,for instance, teams who work on the same schemas should use the same schema prefix. \n",
    "    - Setting the prefix to `dlc_` means that every schema we then create will start with `dlc_` (e.g. `dlc_lab`, `dlc_subject`, `dlc_model` etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "dj.config['database.host'] = '{YOUR_HOST}' \n",
    "dj.config['database.user'] = '{YOUR_USERNAME}' \n",
    "dj.config['database.password'] = getpass.getpass() # enter the password securely\n",
    "dj.config['custom']['database.prefix']= '{YOUR_USERNAME_dlc_}' "
   ]
  },
  {
   "cell_type": "code",
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input arguments are saved in a configuration file named `dj_local_config.json`\n",
    "dj.config.save_local() "
   ]
  },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once set the configuration file, it will be created and saved as `dj_local_conf.json` in the `Element-DeepLabCut directory`. Please, check this file and its content. Remember that this step only needs to be set up once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Configuration code to configure this tutorial in subsequent restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already created and saved the `dj_local_conf.json` file, then you only need to run the following code lines after restart the kernel of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By convention, we set a local config in the workflow directory.\n",
    "import os\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='element-deeplabcut', (\"Please move to the \"\n",
    "                                                              + \"element directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages necessary to run this DataJoint pipeline `Element-DeepLabCut`\n",
    "import datajoint as dj\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design the DataJoint pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the project path specified in the `.json` file, the paths of the input files are extracted to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DLC Project\n",
    "dlc_project_path_abs = Path(dj.config[\"custom\"][\"dlc_root_data_dir\"]) / Path(\n",
    "    dj.config[\"custom\"][\"current_project_folder\"]\n",
    ")  # use pathlib to join; abs path\n",
    "dlc_project_folder = Path(\n",
    "    dj.config[\"custom\"][\"current_project_folder\"]\n",
    ")  # relative path\n",
    "\n",
    "### Config file\n",
    "config_file_abs = dlc_project_path_abs / \"config.yaml\"  # abs path\n",
    "assert (\n",
    "    config_file_abs.exists()\n",
    "), \"Please check the that you have the Top_tracking folder\"\n",
    "\n",
    "### Labeled-data\n",
    "labeled_data_path_abs = dlc_project_path_abs / \"labeled-data\"\n",
    "labeled_files_abs = list(\n",
    "    list(labeled_data_path_abs.rglob(\"*\"))[1].rglob(\"*\")\n",
    ")  # substitute 'training_files'; absolute path\n",
    "labeled_files_rel = []\n",
    "for file in labeled_files_abs:\n",
    "    labeled_files_rel.append(\n",
    "        file.relative_to(dlc_project_path_abs)\n",
    "    )  # substitute 'training_files'; relative path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple Elements into a pipeline\n",
    "\n",
    "Each DataJoint Element is a modular set of tables that can be combined into a complete pipeline.\n",
    "\n",
    "Each Element contains one or more modules, and each module declares its own schema in the database. Schemas are conceptually related sets of tables. \n",
    "\n",
    "This tutorial pipeline is assembled from four DataJoint Elements.\n",
    "\n",
    "| Element | Source Code | Documentation | Description |\n",
    "| -- | -- | -- | -- |\n",
    "| Element Lab | [Link](https://github.com/datajoint/element-lab) | [Link](https://datajoint.com/docs/elements/element-lab) | Lab management related information, such as Lab, User, Project, Protocol, Source. |\n",
    "| Element Animal | [Link](https://github.com/datajoint/element-animal) | [Link](https://datajoint.com/docs/elements/element-animal) | General subject meta data, genotype, and surgery information. |\n",
    "| Element Session | [Link](https://github.com/datajoint/element-session) | [Link](https://datajoint.com/docs/elements/element-session) | General information of experimental sessions. |\n",
    "| Element DeepLabCut | [Link](https://github.com/datajoint/element-deeplabcut) | [Link](https://datajoint.com/docs/elements/element-deeplabcut) | DataJoint schemas (Train and Model) for storing and running analysis of markerless pose estimation with DeepLabCut.\n",
    "\n",
    "The Elements are imported and activated within the `tutorial_pipeline` python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import lab, subject, session, train, model  # after creating json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By importing the modules for the first time, the schemas and tables will be created in the database.  \n",
    "- Once created, importing modules will not create schemas and tables again, but the existing schemas/tables can be accessed.\n",
    "- To empty these schemas and tables for introducing new entries, run (uncomment) the following code lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the session in case of rerunning\n",
    "safemode=True # Set to false to turn off confirmation prompts\n",
    "(session.Session & 'subject=\"subject6\"').delete(safemode=safemode)\n",
    "train.TrainingParamSet.delete(safemode=safemode)\n",
    "train.VideoSet.delete(safemode=safemode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Python module (e.g. `subject`) contains a schema object that enables interaction with the schema in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python classes in the module correspond to a table in the database server. We can check also if there is any entry in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the diagram of tables within multiple schemas and their dependencies using `dj.Diagram()`. \n",
    "\n",
    "This is the diagram of the whole data pipeline for this `Element-DeepLabCut`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject) \n",
    "    + dj.Diagram(lab) \n",
    "    + dj.Diagram(session) \n",
    "    + dj.Diagram(model) \n",
    "    + dj.Diagram(train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the main body of this `Element-DeepLabCut`:"
     ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(model) + dj.Diagram(train)"
     ]
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Types\n",
      "\n",
    "There are 5 table types in DataJoint.  Each of these appear in the diagram above.\n",
      "\n",
    "| Table tier | Color and shape | Description |\n",
    "| -- | -- | -- |\n",
    "| Manual table | Green box | Data entered from outside the pipeline, either by hand or with external helper scripts. |\n",
    "| Lookup table | Gray box | Small tables containing general facts and settings of the data pipeline; not specific to any experiment or dataset. |  \n",
    "| Imported table | Blue oval | Data ingested automatically inside the pipeline but requiring access to data outside the pipeline. |\n",
    "| Computed table | Red circle | Data computed automatically entirely inside the pipeline. |\n",
    "| Part table | Plain text | Part tables share the same tier as their master table. |\n",
      "\n",
    "The diagram becomes clear when it's approached as a hierarchy of tables that define the order in which the pipeline expects to receive data in each of the tables. \n",
    "\n",
    "The tables higher up in the diagram such as `subject.Subject()` should be the first to receive data.\n",
      "\n",
    "Data is manually entered into the green rectangular tables with the `insert1()` method.\n",
      "\n",
    "Tables connected by a line depend on entries from the table above it.\n",
    " \n",
    "Tables with a purple oval or red circle will be automatically filled with relevant data\n",
    "  by calling `populate()`. \n"
     ]
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Links\n",
    "\n",
    "- **One-to-one primary**: thick solid line, share the exact same primary key, meaning the child table inherits all the primary key fields from the parent table as its own primary key\n",
    "\n",
    "- **One-to-many primary**: thin solid line, inherit the primary key from the parent table, but have additional field(s) as part of the primary key as well\n",
    "\n",
    "- **Secondary dependency**: dashed line, the child table inherits the primary key fields from parent table as its own secondary attribute"
     ]
    },
    {
   "cell_type": "markdown",
     "metadata": {},
   "source": [
    "\n",
    "## 3. DataJoint Basics\n",
    "\n",
    "DataJoint pipelines can be run with four commands:\n",
    "\n",
    "- `Insert` metadata about a subject, recording session, and \n",
    "  parameters related to processing markerless pose estimation data through DeepLabCut\n",
    "\n",
    "- `Populate` tables with outputs of pose estimation including training parameters and video recordings\n",
    "\n",
    "- `Query` the data from the database\n",
    "\n",
    "- `Fetch` and plot animal position estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert entries into manual tables\n",
    "\n",
    "#### Common Table Functions\n",
    "- `<table>()` show table contents\n",
    "- `heading` shows attribute definitions\n",
    "- `describe()` show table defintiion with foreign key references\n",
    "\n",
    "Let's have a look at the `TrainingTask` table.\n",
    "\n",
    "To know what data to insert into the table, we can view its dependencies and attributes using `.heading` function. Note that `heading` displays all the attributes of the table definition, regardless of whether they are declared in an upstream table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TrainingTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TrainingTask.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the train table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert entries into manual tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will insert example data into the `subject.Subject (dj.Manual)` table (green in diagrams) by providing values as a dictionary or a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject and Session tables\n",
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject6\",\n",
    "        sex=\"F\",\n",
    "        subject_birth_date=\"2020-01-01\",\n",
    "        subject_description=\"hneih_E105\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table. \n",
    "\n",
    "- We can insert in the `session.Session` table by passing a dictionary to the `insert1` method. \n",
    "\n",
    "- We can look at the contents of this table and restrict by a value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_keys = [\n",
    "    dict(subject=\"subject6\", session_datetime=\"2021-06-02 14:04:22\"),\n",
    "    dict(subject=\"subject6\", session_datetime=\"2021-06-03 14:43:10\"),\n",
    "]\n",
    "session.Session.insert(session_keys, skip_duplicates=True)\n",
    "session.Session() & \"session_datetime > '2021-06-01 12:00:00'\" & \"subject='subject6'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train schema and VideoSet tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VideoSet` table in the `train` schema retains records of files generated in the video labeling process (e.g., `h5`, `csv`, `png`). DeepLabCut will refer to the `mat` file located under the `training-datasets` directory.\n",
    "\n",
    "We recommend storing all paths as relative to the root in your config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoset table\n",
    "train.VideoSet.insert1({\"video_set_id\": 0}, skip_duplicates=True)\n",
    "\n",
    "for idx, filename in enumerate(labeled_files_rel):\n",
    "    train.VideoSet.File.insert1(\n",
    "        {\n",
    "            \"video_set_id\": 0, \n",
    "            \"file_id\": idx, \n",
    "            \"file_path\": dlc_project_folder / filename\n",
    "        },\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.VideoSet.File()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we need to add the parameter set (`TrainingParamSet`) of the model training (`train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TrainingParamSet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `params` attribute has to be a dictionary that captures all the items for the DeepLabCut's `train_network` function. At minimum, this is the contents of the project's config file, as well as `suffle` and `trainingsetindex`, which are not included in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will insert these items, load the config contents, and overwrite some defaults, including `maxiters`, to restrict our training iterations to 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the training interations to 5 modifying the default parameters in config.yaml\n",
    "paramset_idx = 0\n",
    "paramset_desc = \"First training test with DLC using shuffle 1 and maxiters = 5\"\n",
    "\n",
    "# default parameters\n",
    "with open(config_file_abs, \"rb\") as y:\n",
    "    config_params = yaml.safe_load(y)\n",
    "config_params.keys()\n",
    "\n",
    "# new parameters\n",
    "training_params = {\n",
    "    \"shuffle\": \"1\",\n",
    "    \"trainingsetindex\": \"0\",\n",
    "    \"maxiters\": \"5\",\n",
    "    \"scorer_legacy\": \"False\",  # For DLC â‰¤ v2.0, include scorer_legacy = True in params\n",
    "    \"maxiters\": \"5\",\n",
    "    \"multianimalproject\": \"False\",\n",
    "}\n",
    "config_params.update(training_params)\n",
    "\n",
    "train.TrainingParamSet.insert_new_params(\n",
    "    paramset_idx=paramset_idx, paramset_desc=paramset_desc, params=config_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add a `TrainingTask`. As a computed table, `ModelTraining` will reference this to start training when calling `populate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TrainingTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingTask table\n",
    "key = {\n",
    "    \"video_set_id\": 0,\n",
    "    \"paramset_idx\": 0,\n",
    "    \"training_id\": 1,\n",
    "    \"project_path\": dlc_project_folder,\n",
    "}\n",
    "train.TrainingTask.insert1(key, skip_duplicates=True)\n",
    "train.TrainingTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inserting the training parameters and the video recordings, the model training can be run and outputs will be stored in `ModelTraining` table.\n",
    "\n",
    "*Note that the following code line will run the model training with DeepLabCut. It will take some minutes if you have installed DeepLabCut in the GPU. However, it will take longer if the installation was in CPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.ModelTraining.populate(display_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.ModelTraining.fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The network is now trained and ready to evaluate. The next step consists of evaluating the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tracking Joints/Body Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` schema uses a lookup table for managing the body parts tracked across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.BodyPart()\n",
    "new_body_parts = [\n",
    "    dict(body_part=\"subject6\", session_datetime=\"2021-06-02 14:04:22\"),\n",
    "    dict(subject=\"subject6\", session_datetime=\"2021-06-03 14:43:10\"),\n",
    "]\n",
    "session.Session.insert(session_keys, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also modify the body parts as desired. For that, we can use helper functions to identify and insert the new body parts from a given DeepLabCut configuration file (`config.yaml`) in the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.BodyPart.extract_new_body_parts(config_file_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_desc=['Body Center', 'Head', 'Base of Tail']\n",
    "model.BodyPart.insert_from_config(config_file_abs,bp_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Declaring/Evaluating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert into `Model` table for automatic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Model.insert_new_model(model_name='FromTop-latest',dlc_config=config_path,\n",
    "                             shuffle=1,trainingsetindex=0,\n",
    "                             model_description='FromTop - latest snapshot',\n",
    "                             paramset_idx=0,\n",
    "                             params={\"snapshotindex\":-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelEvaluation` will reference the `Model` using the `populate` method and insert the  output from DeepLabCut's `evaluate_network` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ModelEvaluation.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ModelEvaluation.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ModelEvaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our model, we'll first need to insert a session recoring into `VideoRecording`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.VideoRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = {'subject': 'subject6',\n",
    "       'session_datetime': '2021-06-02 14:04:22',\n",
    "       'recording_id': '1', 'device': 'Camera1'}\n",
    "model.VideoRecording.insert1(key)\n",
    "\n",
    "_ = key.pop('device') # get rid of secondary key from master table\n",
    "key.update({'file_id': 1, \n",
    "            'file_path': 'from_top_tracking/videos/test-2s.mp4'})\n",
    "model.VideoRecording.File.insert1(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.VideoRecording.File()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RecordingInfo` automatically populates with file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.RecordingInfo.populate()\n",
    "model.RecordingInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify if the `PoseEstimation` table should load results from an existing file or trigger the estimation command. Here, we can also specify parameters for DeepLabCut's `analyze_videos` as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (model.VideoRecording & {'recording_id': '1'}).fetch1('KEY')\n",
    "key.update({'model_name': 'FromTop-latest', 'task_mode': 'trigger'})\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.PoseEstimationTask.insert_estimation_task(key,params={'save_as_csv':True})\n",
    "model.PoseEstimation.populate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, DataJoint will store results in a subdirectory\n",
    ">       <processed_dir> / videos / device_<name>_recording_<#>_model_<name>\n",
    "where `processed_dir` is optionally specified in the datajoint config. If unspecified, this will be the project directory. The device and model names are specified elsewhere in the schema.\n",
    "\n",
    "We can get this estimation directly as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.PoseEstimation.get_trajectory(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [next notebook](./04-Automate_Optional.ipynb), we'll look at additional tools in the workflow for automating these steps."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ele')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "d00c4ad21a7027bf1726d6ae3a9a6ef39c8838928eca5a3d5f51f3eb68720410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
