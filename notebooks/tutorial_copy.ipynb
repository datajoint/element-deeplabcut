{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DataJoint Element DeepLabCut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open-source Data Pipeline for Markerless Pose Estimation in Neurophysiology**\n",
    "\n",
    "This tutorial focuses on providing a comprehensive understanding of the open-source data pipeline offered by `Element-DeepLabCut`. The package is designed to facilitate pose estimation analyses and streamline the organization of data using `DataJoint`. By the end of this tutorial, participants will have a clear grasp of how to set up, utilize, ad optimize the package for their specific pose estimation projects. \n",
    "\n",
    "**Key Components and Objectives**\n",
    "\n",
    "- 1. Download Sample Data and Context\n",
    "\n",
    "- 2. Setup\n",
    "\n",
    "- 3. Design the DataJoint Pipeline\n",
    "\n",
    "- 4. Enter the Metadata into the Pipeline\n",
    "\n",
    "- 5. Run the Model Training\n",
    "\n",
    "- 6. Run the Model Evaluation\n",
    "\n",
    "\n",
    "For detailed documentation and tutorials on general DataJoint principles that support collaboration, automation, reproducibility, and visualizations:\n",
    "\n",
    "[`DataJoint for Python - Interactive Tutorials`](https://github.com/datajoint/datajoint-tutorials) - Fundamentals including table tiers, query operations, fetch operations, automated computations with the make function, etc.\n",
    "\n",
    "[`DataJoint for Python - Documentation`](https://datajoint.com/docs/core/datajoint-python/0.14/)\n",
    "\n",
    "[`DataJoint Element for DeepLabCut - Documentation`](https://datajoint.com/docs/elements/element-deeplabcut/0.2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Sample Data and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will download the sample data that simulates a real research project. By working through this sample data, you will gain valuable insights into the `practical application` of the package's tools and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Context: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this research project, we are studying the `behavior of a freely-moving mouse in an open-field environment`. The objective is to `extract pose estimations of the animal's head, body, and tail` from video footage. This information can provide valuable insights into the animal's movements, postures, and interactions within the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Sample Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Click the following link to download the sample data archive: `##TO-DO`\n",
    "\n",
    "\n",
    "2. Once the download is complete, extract the contents to a `path of your choice on your local machine`.\n",
    "\n",
    "After running this tutorial, you can try `Element-DeepLabCut` with your own dataset. To do so, create a new `DeepLabCut` folder with your own videos. Then, remember to change the path in the configuration file (`config.yaml`) in your new `DeepLabCut project` folder accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complex Background**: The open field environment introduces complex backgrounds and varying lighting conditions, making accurate pose estimation challenging.\n",
    "\n",
    "**Multiple Body Parts**: Extracting the pose of multiple body parts (head, body, tail) adds complexity to the analysis due to potential occlusions and variations in appearance.\n",
    "\n",
    "**Data Management**: Managing the large volume of video data generated in the field and ensuring consistent annotation requires an efficient data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Outcomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completing this tutorial, you will have acquired practical proficiency in employing the `Element-DeepLabCut` package to effectively tackle the complexities of pose estimation. \n",
    "\n",
    "This tutorial and sample dataset will serve as a practical foundation for your learning journey with the Element package, enabling you to apply these techniques to your own research projects. \n",
    "\n",
    "By integrating this element package with other Elements of DataJoint, you unlock a powerful data pipeline that provides numerous benefits for your research workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Explain this part better and include the link to download the project folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using DataJoint and this tutorial, you need an account to gain access to the database server. \n",
    "\n",
    "Please, go to ### and create an account. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your credentials (DJ_USER, DJ_PASS), you need to connect to the server. To do so, we need to `configure the connection` with the user credentials. \n",
    "\n",
    "- If this is the first time that you are running this tutorial:\n",
    "    - Then you will need to specify the connection parameters by input arguments as in the next subsection `Configuration Code for Initiating this Tutorial`. This section will create a DataJoint configuration file named `dj_local_conf.json` that will save your credentials as environment variables in your local machine. You can find this file in your `Element-Deeplabcut` folder. This configuration file is unique to each machine and DataJoint user.\n",
    "\n",
    "- If you have already run this tutorial and created the `.json` file with your credentials info:\n",
    "    - Then you can directly start from the subsection `Configuration Code to Configure this Tutorial in Subsequent Restarts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration Code for Initiating This Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *The configuration file only needs to be set up once. If you already have one, jump to the following subsection `Configuration Code to Configure this Tutorial in Subsequent Restarts`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='element-deeplabcut', (\"Please move to the \"\n",
    "                                                              + \"element directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the packages necessary to run this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from pathlib import Path\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection parameters are specified by input arguments:\n",
    "- HOST, USER, AND PASSWORD are the fields for the user credentials\n",
    "- Configuring a `custom` field helps manage privileges on a server,for instance, teams who work on the same schemas should use the same schema prefix. \n",
    "    - Setting the prefix to `dlc_` means that every schema we then create will start with `dlc_` (e.g. `dlc_lab`, `dlc_subject`, `dlc_model` etc.)\n",
    "\n",
    "Please, substitute the blue text with your personal host, username, and prefix. Also, your password will be asked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: WHAT HOST IS NECESSARY FOR A NEW USER?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "dj.config['database.host'] = '{YOUR_HOST}' \n",
    "dj.config['database.user'] = '{YOUR_USERNAME}' \n",
    "dj.config['database.password'] = getpass.getpass() # enter the password securely\n",
    "dj.config['custom']['database.prefix']= '{YOUR_USERNAME_dlc_}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DELETE BEFORE COMMIT TO GITHUB\n",
    "\n",
    "import getpass\n",
    "dj.config['database.host'] = 'rds.datajoint.io' \n",
    "dj.config['database.user'] = 'milagrosmarin' \n",
    "dj.config['database.password'] = getpass.getpass() # enter the password securely\n",
    "dj.config['custom']['database.prefix']= 'milagrosmarin_dlc_' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials will be saved and the connection to the database server will be run with the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config.save_local() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the connection to the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once set the configuration file, it will be created and saved as `dj_local_conf.json` in the `Element-DeepLabCut directory`. Please, you may verify this file and its content. Remember that this step only needs to be set up once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Code to Configure this Tutorial in Subsequent Restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already run the previous subsection, the next time you want to run this tutorial (restart the kernel of the notebook) you will only need to start the tutorial from here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='element-deeplabcut', (\"Please move to the \"\n",
    "                                                              + \"element directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the packages necessary to run this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's connect to the database server to be able to use DataJoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Design the DataJoint Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to update the path of your `DeepLabCut project folder` into your configuration file `dj_local_conf.json`. Open the file in your `DeepLabCut-Element` folder, and copy and paste the `DeepLabCut project folder` path in `dlc_root_data_dir`. Also, copy and paste the `DeepLabCut project folder` name in `current_project_folder`:\n",
    "\n",
    "        \"dlc_root_data_dir\": \"{DLC_PROJECT_PATH}\",\n",
    "        \"current_project_folder\": \"{DLC_PROJECT_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can run the following lines to automatically change this information in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from element_interface.utils import find_full_path\n",
    "dj.config.load('dj_local_conf.json')\n",
    "data_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'], # root from config\n",
    "                          'Top_tracking-DataJoint-2023-08-03')       \n",
    "               # DLC project dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the project path specified in the `.json` file, the paths of the input files are charged as variables in this tutorial's session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DLC Project\n",
    "dlc_project_path_abs = Path(dj.config[\"custom\"][\"dlc_root_data_dir\"]) / Path(\n",
    "    dj.config[\"custom\"][\"current_project_folder\"]\n",
    ")  # use pathlib to join; abs path\n",
    "dlc_project_folder = Path(\n",
    "    dj.config[\"custom\"][\"current_project_folder\"]\n",
    ")  # relative path\n",
    "\n",
    "### Config file\n",
    "config_file_abs = dlc_project_path_abs / \"config.yaml\"  # abs path\n",
    "assert (\n",
    "    config_file_abs.exists()\n",
    "), \"Please check the that you have the Top_tracking folder\"\n",
    "\n",
    "### Labeled-data\n",
    "labeled_data_path_abs = dlc_project_path_abs / \"labeled-data\"\n",
    "labeled_files_abs = list(\n",
    "    list(labeled_data_path_abs.rglob(\"*\"))[1].rglob(\"*\")\n",
    ")  # substitute 'training_files'; absolute path\n",
    "labeled_files_rel = []\n",
    "for file in labeled_files_abs:\n",
    "    labeled_files_rel.append(\n",
    "        file.relative_to(dlc_project_path_abs)\n",
    "    )  # substitute 'training_files'; relative path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple Elements into a pipeline\n",
    "\n",
    "Each DataJoint Element is a modular set of tables that can be combined into a complete pipeline.\n",
    "\n",
    "Each Element contains one or more modules, and each module declares its own schema in the database. Schemas are conceptually related sets of tables. \n",
    "\n",
    "This tutorial pipeline is assembled from four DataJoint Elements.\n",
    "\n",
    "| Element | Source Code | Documentation | Description |\n",
    "| -- | -- | -- | -- |\n",
    "| Element Lab | [Link](https://github.com/datajoint/element-lab) | [Link](https://datajoint.com/docs/elements/element-lab) | Lab management related information, such as Lab, User, Project, Protocol, Source. |\n",
    "| Element Animal | [Link](https://github.com/datajoint/element-animal) | [Link](https://datajoint.com/docs/elements/element-animal) | General subject meta data, genotype, and surgery information. |\n",
    "| Element Session | [Link](https://github.com/datajoint/element-session) | [Link](https://datajoint.com/docs/elements/element-session) | General information of experimental sessions. |\n",
    "| Element DeepLabCut | [Link](https://github.com/datajoint/element-deeplabcut) | [Link](https://datajoint.com/docs/elements/element-deeplabcut) | DataJoint schemas (Train and Model) for storing and running analysis of markerless pose estimation with DeepLabCut.\n",
    "\n",
    "The Elements are imported and activated in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import lab, subject, session, train, model  # after creating json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By importing the modules for the first time, the schemas and tables will be created in the database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.list_schemas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, importing modules will not create schemas and tables again, but the existing schemas/tables can be accessed.\n",
    "To empty these schemas and tables for introducing new entries, run (uncomment) the following code lines (note that you will have to commit the delete in the prompt by typing \"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the session in case of rerunning\n",
    "safemode=True # Set to false to turn off confirmation prompts\n",
    "session.Session.delete(safemode=safemode)\n",
    "train.TrainingParamSet.delete(safemode=safemode)\n",
    "train.VideoSet.delete(safemode=safemode)\n",
    "model.BodyPart.delete(safemode=safemode)\n",
    "subject.Subject.delete(safemode=safemode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Python module (e.g. `subject`) contains a schema object that enables interaction with the schema in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python classes in the module correspond to a table in the database server. We can check also if there is any entry in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the diagram of the whole data pipeline for this `Element-DeepLabCut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject) \n",
    "    + dj.Diagram(lab) \n",
    "    + dj.Diagram(session) \n",
    "    + dj.Diagram(model) \n",
    "    + dj.Diagram(train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the main body of this `Element-DeepLabCut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(model) + dj.Diagram(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enter the Metadata into the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the `Model Training`, we need to start by adding the input data to the `train` module. Let's start having a look at the `TrainingTask` table. This table will pair each video set with their corresponding training parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TrainingTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pair some example data and launch training via `process`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IS THIS NEEDED???\n",
    "\n",
    "#key={'paramset_idx':0,'training_id':0,'video_set_id':0, \n",
    "#     'project_path':dlc_project_folder}\n",
    "#train.TrainingTask.insert1(key, skip_duplicates=True)\n",
    "#process.run(verbose=True, display_progress=True)\n",
    "#model.RecordingInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Subject` module corresponds to the table that will contain the subject (e.g., the mouse) information. Let's insert example entries into the `subject.Subject` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject and Session tables\n",
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject6\",\n",
    "        sex=\"F\",\n",
    "        subject_birth_date=\"2020-01-01\",\n",
    "        subject_description=\"hneih_E105\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the step for the `Session` module. We can also insert in the `Session` table by passing a dictionary to the `insert1` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of the dictionary named \"session_keys\"\n",
    "session_keys = [\n",
    "    dict(subject=\"subject6\", session_datetime=\"2021-06-02 14:04:22\"),\n",
    "    dict(subject=\"subject6\", session_datetime=\"2021-06-03 14:43:10\"),\n",
    "]\n",
    "\n",
    "#Insert this dictionary in the Session table\n",
    "session.Session.insert(session_keys, skip_duplicates=True)\n",
    "session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VideoSet` table in the `train` schema retains records of files generated in the video labeling process (e.g., `h5`, `csv`, `png`). DeepLabCut will refer to the `mat` file located under the `training-datasets` directory.\n",
    "\n",
    "We recommend storing all paths as relative to the root in your config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videoset table \n",
    "train.VideoSet.insert1({\"video_set_id\": 0}, skip_duplicates=True)\n",
    "\n",
    "for idx, filename in enumerate(labeled_files_rel):\n",
    "    train.VideoSet.File.insert1(\n",
    "        {\n",
    "            \"video_set_id\": 0, \n",
    "            \"file_id\": idx, \n",
    "            \"file_path\": dlc_project_folder / filename\n",
    "        },\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.VideoSet.File()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we need to add the parameter set (`TrainingParamSet`) of the model training (`train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TrainingParamSet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `params` attribute has to be a dictionary that captures all the items for the DeepLabCut's `train_network` function. At minimum, this is the contents of the project's config file, as well as `suffle` and `trainingsetindex`, which are not included in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will insert these items, load the config contents, and overwrite some defaults, including `maxiters`, to restrict our training iterations to 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the training interations to 5 modifying the default parameters in config.yaml\n",
    "paramset_idx = 0\n",
    "paramset_desc = \"First training test with DLC using shuffle 1 and maxiters = 5\"\n",
    "\n",
    "# default parameters\n",
    "with open(config_file_abs, \"rb\") as y:\n",
    "    config_params = yaml.safe_load(y)\n",
    "config_params.keys()\n",
    "\n",
    "# new parameters\n",
    "training_params = {\n",
    "    \"shuffle\": \"1\",\n",
    "    \"trainingsetindex\": \"0\",\n",
    "    \"maxiters\": \"5\",\n",
    "    \"scorer_legacy\": \"False\",  # For DLC â‰¤ v2.0, include scorer_legacy = True in params\n",
    "    \"maxiters\": \"5\",\n",
    "    \"multianimalproject\": \"False\",\n",
    "}\n",
    "config_params.update(training_params)\n",
    "\n",
    "train.TrainingParamSet.insert_new_params(\n",
    "    paramset_idx=paramset_idx, paramset_desc=paramset_desc, params=config_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add a `TrainingTask`. As a computed table, `ModelTraining` will reference this to start training when calling `populate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TrainingTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingTask table\n",
    "key = {\n",
    "    \"video_set_id\": 0,\n",
    "    \"paramset_idx\": 0,\n",
    "    \"training_id\": 1,\n",
    "    \"project_path\": dlc_project_folder,\n",
    "}\n",
    "train.TrainingTask.insert1(key, skip_duplicates=True)\n",
    "train.TrainingTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inserting the training parameters and the video recordings, the model training can be run and outputs will be stored in `ModelTraining` table.\n",
    "\n",
    "*Note that the following code line will run the model training with DeepLabCut. It will take some minutes if you have installed DeepLabCut in the GPU. However, it will take longer if the installation was in CPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.ModelTraining.populate(display_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.ModelTraining.fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The network is now trained and ready to evaluate. The next step consists of evaluating the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tracking Joints/Body Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` schema uses a lookup table for managing the body parts tracked across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.BodyPart()\n",
    "new_body_parts = [\n",
    "    dict(body_part=\"subject6\", session_datetime=\"2021-06-02 14:04:22\"),\n",
    "    dict(subject=\"subject6\", session_datetime=\"2021-06-03 14:43:10\"),\n",
    "]\n",
    "session.Session.insert(session_keys, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also modify the body parts as desired. For that, we can use helper functions to identify and insert the new body parts from a given DeepLabCut configuration file (`config.yaml`) in the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.BodyPart.extract_new_body_parts(config_file_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ONLY if there are new body parts compared to the config.yaml. If the table has already descriptions, then leave it empty.\n",
    "bp_desc=[]\n",
    "model.BodyPart.insert_from_config(config_file_abs,bp_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Declaring/Evaluating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert into `Model` table for automatic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Model.insert_new_model(model_name='FromTop-latest',\n",
    "                             dlc_config=config_file_abs,\n",
    "                             shuffle=1,\n",
    "                             trainingsetindex=0,\n",
    "                             model_description='FromTop - latest snapshot',\n",
    "                             paramset_idx=0,\n",
    "                             params={\"snapshotindex\":-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.BodyPart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelEvaluation` will reference the `Model` using the `populate` method and insert the  output from DeepLabCut's `evaluate_network` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ModelEvaluation.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ModelEvaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our model, we'll first need to insert a session recording into `VideoRecording`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.VideoRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = {'subject': 'subject6',\n",
    "       'session_datetime': '2021-06-02 14:04:22',\n",
    "       'recording_id': '1', 'device': 'Camera1'}\n",
    "model.VideoRecording.insert1(key, skip_duplicates=True)\n",
    "\n",
    "_ = key.pop('device') # get rid of secondary key from master table // why this step???\n",
    "key.update({'file_id': 1, \n",
    "            'file_path': 'Top_tracking-DataJoint-2023-08-03/videos/train1_trimmed.mp4'})\n",
    "model.VideoRecording.File.insert1(key, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.VideoRecording.File()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RecordingInfo` automatically populates with file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.RecordingInfo.populate()\n",
    "model.RecordingInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify if the `PoseEstimation` table should load results from an existing file or trigger the estimation command. Here, we can also specify parameters for DeepLabCut's `analyze_videos` as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_dict = (model.VideoRecording & {\"recording_id\": \"1\"}).fetch1(\"KEY\")\n",
    "recording_dict.update({\"model_name\": \"FromTop-latest\", \"task_mode\": \"trigger\"})\n",
    "# videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve\n",
    "analyze_videos_params = {\"save_as_csv\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, DataJoint will store results in a subdirectory\n",
    ">       <processed_dir> / videos / device_<name>_recording_<#>_model_<name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`processed_dir` is optionally specified in the datajoint config, or in the `insert_estimation_task`. If unspecified, this will be the project directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.PoseEstimationTask.infer_output_dir(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.PoseEstimationTask.insert_estimation_task(recording_dict, model_name = recording_dict[\"model_name\"], analyze_videos_params=analyze_videos_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.PoseEstimationTask.insert_estimation_task(key,params={'save_as_csv':True})\n",
    "model.PoseEstimation.populate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting coordinates of the pose estimation are now available in the corresponding `BodyPartPosition` table, ready to use for visualization, or to combine with other Elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.PoseEstimation.BodyPartPosition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the pose estimation results directly as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.PoseEstimation.coordinates_dataframe(key)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ele')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "d00c4ad21a7027bf1726d6ae3a9a6ef39c8838928eca5a3d5f51f3eb68720410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
